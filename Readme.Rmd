---
title: 'Supplementary Information: Older participants are slower in a visual lexical decision task, but this is attenuated by a large vocabulary'
author: "Rácz, Péter"
date: "`r format(Sys.Date(),'%e %B, %Y')`"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, error = FALSE, message = FALSE, fig.path = 'figures/', dpi=300, fig.width = 8, fig.height = 4)
knitr::opts_knit$set(root.dir = '~/Github/RaczLukacs2024')

setwd('~/Github/RaczLukacs2024')

library(tidyverse)
library(magrittr)
library(glue)
library(knitr)
library(ggthemes)
library(patchwork)
library(sjPlot)
library(broom.mixed)
library(lme4)

source('helper.R')

```

## Links

The code to run the experiments on Gitlab for pilot 1, pilot 2, and the main experiment are [**here**](https://gitlab.pavlovia.org/petyaraczbme/lex_span4), [**here**](https://gitlab.pavlovia.org/petyaraczbme/lendulet_bme_szokiserlet_vegso), and [**here**](https://gitlab.pavlovia.org/petyaraczbme/lex-dec-task-random).

## File structure and workflow

We ran two pilot experiments and a main experiment. The stimulus list for pilot 1 was based on a frequency list from [Hungarian Webcorpus 2](https://hlt.bme.hu/en/resources/webcorpus2). We host this list in this repository. Cite us and the original if you want to use it. Subsequent stimulus lists were based on results from pilot 1.

- src: corpus data in various processed forms
- raw: raw output of the Gitlab scripts running the experiments
- scripts: scripts to create the word lists, process the raw data, create the tidy data, and check on the processing
- tidy: tidy data for pilot 1, 2, and main
- analysis: scripts to analyse the tidy data from the main experiment

## Data dictionary for main data (tidy/d.tsv)

 - id_spec: participant id with timestamp
 - n_mistakes_for_nonce_words: n times participant said yes to non-word
 - percent_mistakes_for_nonce_words: % participant said yes to non-word
 - id: participant id
 - data_type: did participant finish exp (they all did)
 - participant_cap_2: counting participant vocab size v2
 - participant_cap_1:  counting participant vocab size v1
 - max_trial_number: 250 for everyone
 - max_block: 50 for everyone
 - n_mistakes_for_real_words_total
 - block
 - n_mistakes_for_real_words_per_bin:n times part said no to real word
 - yob: year of birth
 - edu: years spent in education
 - start: starting experiment
 - exp: this is exp3
 - row_number: row number in data
 - block_trial_number: trial n 1-250 (only one block)
 - trial_number: trial n 1-250
 - word: target
 - resp.keys: what key did part press
 - resp.rt: part time <- this is the outcome variable
 - pos: target part of speech
 - bin: target familiarity bin
 - lfpm10r: log freq per 10 mil in Webcorpus 2, scaled
 - nonce_word: this is a non-word
 - correct: correct answer (y for word, n for non-word)
 - link: link to exp
 - missed_nonce_words: n non-words missed by part
 - gender: part gender, self reported
 - drop_participant: part meets exclusion criteria
 - drop_observation: response meets exclusion criteria
 - year_of_birth: year of birth tidy
 - start_time: start time tidy
 - start_year: start year
 - age: year - yob
 - answer: yes or no
 - participant_age: age <- predictor
 - participant_vocabulary_size: vocab size <- predictor
 - word_familiarity: word familiarity bin <- predictor
 
## Exclusion criteria

- drop_participant = missed_nonce_words > 20
- drop_observation = resp.rt > 4

```{r counts1, echo = T}
# n obs
nrow(all)
# n part
length(unique(all$id_spec))
# n filt obs
nrow(filt)
# n filt part
length(unique(filt$id_spec))
# filt counts
filt |> 
  count(nonce_word,correct) |> 
  pivot_wider(names_from = nonce_word, values_from = n) |> 
  rename('non word' = `TRUE`, 'real word' = `FALSE`) |> 
  mutate(response = ifelse(correct, 'yes', 'no')) |> 
  select(response,`real word`,`non word`) |> 
  kable('simple')
# young
nrow(idsy)
# old
nrow(idso)
```

## Model comparison

Three models fit on young and old data. lme4. ML. for main and resid models, participant and word random intercept plus word familiarity slope for participant. AIC, BIC, likelihood ratio test used for model comparison. Models checked for collinearity using variance inflation factor. (age and vocab size very collinear, so we decided not to test interactions directly).

Young data:
- Vocabulary: Is the vocabulary x age relationship linear or polynomial?
- Main: RT ~ part age, vocab size, word familiarity
- Resid Vocab/Age: RT ~ part age, variation in vocab size not explained by age (residualised vocabulary size), word familiarity

```{r models1, echo = T}
# best models:

## -- young -- ##

## vocabulary
myv1 = lm(s_size ~ s_age, data = idsy)
## main
mys1 = lmer(resp.rt ~ 1 + s_age + s_word + s_size + (1+s_word|id) + (1|word), data = young, REML = F, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=20000)))
## residualised
myr1 = lmer(resp.rt ~ 1 + s_age + res_size_age + s_word + (1+s_word|id) + (1|word), data = young, REML = F, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=20000)))
```

Young data:
- Vocabulary: Is the vocabulary x age relationship linear or polynomial?
- Main: RT ~ part age, vocab size, education, word familiarity
- Resid Vocab/Age: RT ~ part edu, age, variation in vocab size not explained by age (residualised vocabulary size), word familiarity
- Resid Vocab/Edu: RT ~ part age, edu, variation in vocab size not explained by edu (residualised vocabulary size), word familiarity

```{r models2, echo = T}
# best models ctd.:

## -- old -- ##

## vocabulary
mov1 = lm(s_size ~ 1 + s_edu + s_age, data = idso)
## main
mos1 = lmer(resp.rt ~ 1 + s_age + s_edu + s_word + s_size + (1+s_word|id) + (1|word), data = old, REML = F, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=20000)))
## residualised
mor3 = lmer(resp.rt ~ 1 + s_edu + s_word + res_size_age * s_age + (1+s_word|id) + (1|word), data = old, REML = F, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=20000)))
mor4 = lmer(resp.rt ~ 1 + s_age + s_word + res_size_edu * s_edu + (1+s_word|id) + (1|word), data = old, REML = F, control = lmerControl(optimizer="bobyqa", optCtrl=list(maxfun=20000)))
```  

## Tables in paper

```{r tables1}
mys1 |> 
  tidy(conf.int = T) |>
  filter(effect == 'fixed') |> 
  select(term,estimate,std.error,statistic,conf.low,conf.high) |> 
  # kable('latex', digits = 2)
  kable('simple', digits = 2)
# 
# myr1 |> 
#   tidy(conf.int = T) |>
#   filter(effect == 'fixed') |> 
#   select(term,estimate,std.error,statistic,conf.low,conf.high) |> 
#   # kable('latex', digits = 2)
#   kable('simple', digits = 2)

mos1 |> 
  tidy(conf.int = T) |>
  filter(effect == 'fixed') |>
  select(term,estimate,std.error,statistic,conf.low,conf.high) |>
  # kable('latex', digits = 2)
  kable('simple', digits = 2)

mor3 |> 
  tidy(conf.int = T) |>
  filter(effect == 'fixed') |>
  select(term,estimate,std.error,statistic,conf.low,conf.high) |>
  # kable('latex', digits = 2)
  kable('simple', digits = 2)

mor4 |> 
  tidy(conf.int = T) |>
  filter(effect == 'fixed') |>
  select(term,estimate,std.error,statistic,conf.low,conf.high) |>
  # kable('latex', digits = 2)
  kable('simple', digits = 2)
```

## Plots in paper

```{r plot1, width = 6, height = 3}
sums = d |> 
  summarise(mean = mean(resp.rt), .by = c(id_spec,participant_age,participant_vocabulary_size,participant_education))

sums |> 
  ggplot(aes(participant_age,mean)) +
  geom_point() +
  geom_smooth(method = 'gam') +
  theme_bw() +
  geom_vline(xintercept = 18, lty = 3) +
  ylim(0,3.2) +
  xlab('participant age (years)') +
  ylab('participant mean RT') + 
  annotate("text", x = 30, y = 2.2, label = "← 18 years")
ggsave('figures/plot1-1.pdf', width = 6, height = 3)

p1 = sums |> 
  ggplot(aes(participant_age,participant_vocabulary_size)) +
  geom_point() +
  geom_smooth(method = 'gam') +
  theme_bw() +
  xlab('participant age (years)') +
  ylab('participant vocabulary size (1-50)')

p2 = sums |> 
  ggplot(aes(participant_age,participant_education)) +
  geom_point() +
  geom_smooth(method = 'gam') +
  theme_bw() +
  xlab('participant age (years)') +
  ylab('participant years spent in education')

p3 = sums |> 
  ggplot(aes(participant_education,participant_vocabulary_size)) +
  geom_point() +
  geom_smooth(method = 'gam') +
  theme_bw() +
  xlab('participant years\nspent in education') +
  ylab('participant vocabulary size (1-50)')

p1 + p2 + p3
ggsave('figures/plot1-2.pdf', width = 8, height = 3)

plot_model(mor3, 'pred', terms = c('s_age','res_size_age')) +
  scale_fill_viridis_d() +
  scale_colour_viridis_d() +
  theme_bw() +
  labs(colour = 'residual\nvocabulary\nsize') +
  xlab('participant age (scaled)') +
  ylab('response time (s)') +
  ggtitle('')
ggsave('figures/plot1-3.pdf', width = 5, height = 3)

plot_model(mor4, 'pred', terms = c('s_edu','res_size_edu')) +
  scale_fill_viridis_d(option = 'H') +
  scale_colour_viridis_d(option = 'H') +
  theme_bw() +
  labs(colour = 'residual\nvocabulary\nsize') +
  xlab('participant education (scaled)') +
  ylab('response time (s)') +
  ggtitle('')
ggsave('figures/plot1-4.pdf', width = 5, height = 3)

```
